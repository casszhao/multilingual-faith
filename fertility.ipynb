{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_abb = {\"bert-base-multilingual-uncased\":\"mbert\",\n",
    "                            \"xlm-roberta-base\":\"xlm_roberta\",\n",
    "                            \"xlm-roberta-large\":\"xlm_roberta_large\",\n",
    "                            \"microsoft/deberta-base\":\"deberta\",\n",
    "                            \"google/mt5-small\": \"t5m\",\n",
    "        \n",
    "        \n",
    "                            \"bert-base-uncased\": \"bert\", \n",
    "                            \"bert-large-uncased\": \"bert_large\", \n",
    "                            \"roberta-base\": \"roberta\",\n",
    "                            \"roberta-large\": \"roberta_large\",\n",
    "\n",
    "                             \"bert-base-chinese\":\"chinese_bert\",\n",
    "                             \"hfl/chinese-macbert-base\":\"macbert\",\n",
    "                             \"hfl/chinese-macbert-large\": \"macbert_large\",\n",
    "                             \"hfl/chinese-roberta-wwm-ext\": \"chinese_roberta\",\n",
    "                             \"hfl/chinese-roberta-wwm-ext-large\": \"chinese_roberta_large\",\n",
    "                             \"benjamin/roberta-base-wechsel-chinese\": \"chinese_roberta_wechsel\",\n",
    "\n",
    "                             \"ClassCat/roberta-base-french\": \"french_roberta\",\n",
    "                             \"dbmdz/bert-base-french-europeana-cased\": \"french_bert\",\n",
    "                             \"flaubert/flaubert_base_uncased\": \"flaubert\",\n",
    "                             \"benjamin/roberta-base-wechsel-french\": \"french_roberta_wechsel\",\n",
    "\n",
    "                             \"dccuchile/albert-base-spanish\": \"ALBETO\",\t\n",
    "                             \"dccuchile/bert-base-spanish-wwm-uncased\": \"BETO\",\n",
    "                             \"dccuchile/distilbert-base-spanish-uncased\": \"spanish_distilbert\",\n",
    "                             \"PlanTL-GOB-ES/roberta-base-bne\":\"spanish_roberta\",\n",
    "\n",
    "                             \"l3cube-pune/hindi-bert-scratch\": \"hindi_bert\",\n",
    "                             \"flax-community/roberta-hindi\": \"hindi_roberta\",\n",
    "\n",
    "\n",
    "                            \"aubmindlab/bert-base-arabertv02\": \"arabic_bert\",\n",
    "                            \"sultan/ArabicT5-xLarge\": \"arabic_tfive\"\n",
    "\n",
    "                                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cass/anaconda3/envs/seq/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-02 16:04:15.451764: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-02 16:04:15.724098: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-02 16:04:16.599829: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_data_list = ['sst', 'agnews', 'multirc']\n",
    "hindi_data_list = ['hindi_xnli', 'hindi_bbc_nli', 'hindi_bbc_topic']\n",
    "chinese_data_list = ['ChnSentiCorp', 'csl', 'ant']  # chinese_xnli csl\n",
    "spanish_data_list = ['spanish_csl', 'spanish_paws', 'spanish_xnli']\n",
    "french_data_list = ['french_csl', 'french_paws', 'french_xnli']\n",
    "\n",
    "language_data_dict = {'English': english_data_list, \n",
    "                      'Hindi': hindi_data_list,\n",
    "                      'Chinese': chinese_data_list,\n",
    "                      'Spanish': spanish_data_list,\n",
    "                      'French': french_data_list,\n",
    "                      }\n",
    "\n",
    "all_data_list = english_data_list+chinese_data_list+spanish_data_list+french_data_list+hindi_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_model_list = [\"bert-base-multilingual-uncased\", \"xlm-roberta-base\"]\n",
    "chinese_model_list = [\"bert-base-chinese\", \"hfl/chinese-roberta-wwm-ext\"]\n",
    "hindi_model_list = [\"l3cube-pune/hindi-bert-scratch\", \"flax-community/roberta-hindi\"]\n",
    "spanish_model_list = [\"dccuchile/bert-base-spanish-wwm-uncased\",\"PlanTL-GOB-ES/roberta-base-bne\"]\n",
    "french_model_list = [\"benjamin/roberta-base-wechsel-chinese\", \"dbmdz/bert-base-french-europeana-cased\", \"ClassCat/roberta-base-french\"]\n",
    "english_model_list = [\"bert-base-uncased\", \"roberta-base\"]\n",
    "# \n",
    "\n",
    "# query list means these datasets do not have column text\n",
    "chinese_query_list = ['csl', 'chinese_xnli', 'ant']\n",
    "english_query_list = ['multirc']\n",
    "french_query_list = ['french_paws'] # , 'french_xnli'\n",
    "spanish_query_list = ['spanish_paws'] # , 'spanish_xnli'\n",
    "hindi_query_list = ['hindi_bbc_nli'] # 'hindi_xnli', \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 450/450 [00:00<00:00, 457kB/s]\n",
      "Downloading vocab.json: 100%|██████████| 1.45M/1.45M [00:00<00:00, 6.45MB/s]\n",
      "Downloading merges.txt: 100%|██████████| 1.11M/1.11M [00:00<00:00, 3.69MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 2.67M/2.67M [00:00<00:00, 7.80MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 171kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"benjamin/roberta-base-wechsel-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length = 517) \n",
    "\n",
    "tokenized_french = tokenizer.tokenize(\"बर्मा में मुसलमानों के खिलाफ हिंसा के और सुबूत\")\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 2, 2], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tokenizer(\"</s>\")\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 24302, 110, 24302, 113, 159, 103, 226, 24302, 111, 24302, 127, 211, 24302, 111, 24302, 121, 159, 103, 214, 24302, 110, 159, 103, 215, 24302, 102, 2, 2, 24302, 111, 159, 103, 214, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tokenizer(\"बर्मा मसुबूत\", \"मु\")\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaTokenizerFast(name_or_path='xlm-roberta-base', vocab_size=250002, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_name = \"xlm-roberta-base\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length = 517) \n",
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Très',\n",
       " '▁bon',\n",
       " '▁produit',\n",
       " ',',\n",
       " '▁le',\n",
       " '▁meilleur',\n",
       " '▁film',\n",
       " '▁de',\n",
       " '▁protection',\n",
       " '▁d',\n",
       " \"'\",\n",
       " 'écran',\n",
       " '▁jamais',\n",
       " '▁acheté',\n",
       " '▁pour',\n",
       " '▁mes',\n",
       " '▁smartphones',\n",
       " '.',\n",
       " '▁Le',\n",
       " '▁produit',\n",
       " '▁n',\n",
       " \"'\",\n",
       " 'est',\n",
       " '▁pas',\n",
       " '▁en',\n",
       " '▁verre',\n",
       " ',',\n",
       " '▁c',\n",
       " \"'\",\n",
       " 'est',\n",
       " '▁un',\n",
       " '▁film',\n",
       " '▁soup',\n",
       " 'le',\n",
       " '▁d',\n",
       " \"'\",\n",
       " 'appar',\n",
       " 'ence',\n",
       " '▁très',\n",
       " '▁ré',\n",
       " 'si',\n",
       " 'stant',\n",
       " '.',\n",
       " '▁Par',\n",
       " 'fa',\n",
       " 'ite',\n",
       " 'ment',\n",
       " '▁trans',\n",
       " 'luci',\n",
       " 'de',\n",
       " '▁(',\n",
       " 'impression',\n",
       " 'nant',\n",
       " ')',\n",
       " '▁comme',\n",
       " '▁du',\n",
       " '▁cristal',\n",
       " '.',\n",
       " '▁Les',\n",
       " '▁marques',\n",
       " '▁de',\n",
       " '▁doigts',\n",
       " '▁sont',\n",
       " '▁minime',\n",
       " 's',\n",
       " '.',\n",
       " '▁Pos',\n",
       " 'é',\n",
       " '▁facilement',\n",
       " '▁et',\n",
       " '▁dès',\n",
       " '▁la',\n",
       " '▁1',\n",
       " 'ère',\n",
       " '▁tentative',\n",
       " '.',\n",
       " '▁Dimension',\n",
       " 's',\n",
       " '▁parfaitement',\n",
       " '▁adapté',\n",
       " '▁au',\n",
       " '▁Honor',\n",
       " '▁6',\n",
       " 'X',\n",
       " '▁dont',\n",
       " '▁le',\n",
       " '▁film',\n",
       " '▁d',\n",
       " \"'\",\n",
       " 'origine',\n",
       " '▁avait',\n",
       " '▁son',\n",
       " '▁temps',\n",
       " '▁après',\n",
       " '▁18',\n",
       " '▁mois',\n",
       " '▁d',\n",
       " \"'\",\n",
       " 'utilisation',\n",
       " '▁de',\n",
       " '▁bons',\n",
       " '▁et',\n",
       " '▁lo',\n",
       " 'y',\n",
       " 'aux',\n",
       " '▁services',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_name = \"xlm-roberta-base\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length = 517) \n",
    "# tokenizer\n",
    "\n",
    "# tokenized_french = tokenizer.tokenize(\"Très bon produit, le meilleur film de protection d'écran jamais acheté pour mes smartphones. Le produit n'est pas en verre, c'est un film souple d'apparence très résistant. Parfaitement translucide (impressionnant) comme du cristal. Les marques de doigts sont minimes. Posé facilement et dès la 1ère tentative. Dimensions parfaitement adapté au Honor 6X dont le film d'origine avait son temps après 18 mois d'utilisation de bons et loyaux services.\")\n",
    "# tokenized_french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_token = tokenizer.special_tokens_map.get('unk_token')\n",
    "unknown_id = tokenizer.convert_tokens_to_ids(unknown_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_english(string):\n",
    "    pattern = r'[a-zA-Z0-9\\s!\"#$%&\\'()*+,-./:;<=>?@\\[\\\\\\]^_`{|}~]'\n",
    "    return re.sub(pattern, '', string.replace(\" \", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> tex: if you sometimes like to go to the movies to have fun wasabi is a good place to start\n"
     ]
    }
   ],
   "source": [
    "testing_text_list = pd.read_csv('datasets/sst/data/test.csv')['text']\n",
    "t = testing_text_list[:3]\n",
    "\n",
    "# model_name = \"bert-base-uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length = 517) \n",
    "\n",
    "\n",
    "# tex = t[0]\n",
    "# print(f\"==>> tex: {tex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0\n",
    "\n",
    "for i, t in enumerate(testing_text_list):\n",
    "    if i == 0: print(t)\n",
    "    \n",
    "    text_length = len(t.split(' '))\n",
    "\n",
    "    tokenized_len = len(tokenizer.tokenize(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> one_text_id: [101, 11526, 10855, 16933, 11531, 10114, 11335, 10114, 10103, 25194, 10114, 10574, 22271, 10140, 64246, 10127, 143, 12050, 11125, 10114, 13982, 102]\n",
      "==>> tokenized: ['if', 'you', 'sometimes', 'like', 'to', 'go', 'to', 'the', 'movies', 'to', 'have', 'fun', 'was', '##abi', 'is', 'a', 'good', 'place', 'to', 'start']\n",
      "==>> one_text_id: [0, 2174, 398, 68018, 1884, 47, 738, 47, 70, 72304, 47, 765, 7477, 509, 14508, 83, 10, 4127, 3687, 47, 4034, 2]\n",
      "==>> tokenized: ['▁if', '▁you', '▁sometimes', '▁like', '▁to', '▁go', '▁to', '▁the', '▁movies', '▁to', '▁have', '▁fun', '▁was', 'abi', '▁is', '▁a', '▁good', '▁place', '▁to', '▁start']\n"
     ]
    }
   ],
   "source": [
    "multi_model_list = [\"bert-base-multilingual-uncased\", \"xlm-roberta-base\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\", model_max_length = 517) \n",
    "unknown_token = tokenizer.special_tokens_map.get('unk_token')\n",
    "unknown_id = tokenizer.convert_tokens_to_ids(unknown_token)\n",
    "\n",
    "one_text_id = tokenizer(tex)['input_ids']\n",
    "print(f\"==>> one_text_id: {one_text_id}\")\n",
    "\n",
    "tokenized = tokenizer.tokenize(tex)\n",
    "print(f\"==>> tokenized: {tokenized}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\", model_max_length = 517) \n",
    "unknown_token = tokenizer.special_tokens_map.get('unk_token')\n",
    "unknown_id = tokenizer.convert_tokens_to_ids(unknown_token)\n",
    "\n",
    "one_text_id = tokenizer(tex)['input_ids']\n",
    "print(f\"==>> one_text_id: {one_text_id}\")\n",
    "\n",
    "tokenized = tokenizer.tokenize(tex)\n",
    "print(f\"==>> tokenized: {tokenized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> one_text_id: [101, 1690, 1766, 1698, 4243, 1784, 3202, 6740, 3199, 5975, 3568, 7969, 8956, 7658, 10032, 7516, 7515, 7171, 2481, 1659, 6990, 5836, 102]\n",
      "==>> tokenized: ['为', '什', '么', '支', '付', '宝', '绑', '定', '的', '建', '设', '银', '行', '，', '蚂', '蚁', '花', '呗', '不', '能', '用']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\", model_max_length = 517) \n",
    "unknown_token = tokenizer.special_tokens_map.get('unk_token')\n",
    "unknown_id = tokenizer.convert_tokens_to_ids(unknown_token)\n",
    "\n",
    "one_text_id = tokenizer(tex)['input_ids']\n",
    "print(f\"==>> one_text_id: {one_text_id}\")\n",
    "\n",
    "tokenized = tokenizer.tokenize(tex)\n",
    "print(f\"==>> tokenized: {tokenized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> one_text_id: [101, 711, 784, 720, 3118, 802, 2140, 5308, 2137, 4638, 2456, 6392, 7213, 6121, 8024, 6010, 6009, 5709, 1446, 679, 5543, 4500, 102]\n",
      "==>> tokenized: ['为', '什', '么', '支', '付', '宝', '绑', '定', '的', '建', '设', '银', '行', '，', '蚂', '蚁', '花', '呗', '不', '能', '用']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\", model_max_length = 517) \n",
    "unknown_token = tokenizer.special_tokens_map.get('unk_token')\n",
    "unknown_id = tokenizer.convert_tokens_to_ids(unknown_token)\n",
    "\n",
    "one_text_id = tokenizer(tex)['input_ids']\n",
    "print(f\"==>> one_text_id: {one_text_id}\")\n",
    "\n",
    "tokenized = tokenizer.tokenize(tex)\n",
    "print(f\"==>> tokenized: {tokenized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> one_text_id: [101, 711, 784, 720, 3118, 802, 2140, 5308, 2137, 4638, 2456, 6392, 7213, 6121, 8024, 6010, 6009, 5709, 1446, 679, 5543, 4500, 102]\n",
      "==>> tokenized: ['为', '什', '么', '支', '付', '宝', '绑', '定', '的', '建', '设', '银', '行', '，', '蚂', '蚁', '花', '呗', '不', '能', '用']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\", model_max_length = 517) \n",
    "unknown_token = tokenizer.special_tokens_map.get('unk_token')\n",
    "unknown_id = tokenizer.convert_tokens_to_ids(unknown_token)\n",
    "\n",
    "one_text_id = tokenizer(tex)['input_ids']\n",
    "print(f\"==>> one_text_id: {one_text_id}\")\n",
    "\n",
    "tokenized = tokenizer.tokenize(tex)\n",
    "print(f\"==>> tokenized: {tokenized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (830 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChnSentiCorp\n",
      "csl\n",
      "ant\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xlm_roberta_UnkNum</th>\n",
       "      <th>xlm_roberta_UnkRatio</th>\n",
       "      <th>xlm_roberta_Fertility</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ChnSentiCorp</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.748148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csl</th>\n",
       "      <td>446</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.818059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ant</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.814438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              xlm_roberta_UnkNum  xlm_roberta_UnkRatio  xlm_roberta_Fertility\n",
       "Dataset                                                                      \n",
       "ChnSentiCorp                   5              0.000055               0.748148\n",
       "csl                          446              0.000795               0.818059\n",
       "ant                            1              0.000018               0.814438"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_one_lan_one_model(model, language_data_list, query_list, if_chinese=False):\n",
    "    unknow_ratio = []\n",
    "    unknow_num = []\n",
    "    fertility = []\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model, model_max_length = 517) \n",
    "    unknown_token = tokenizer.special_tokens_map.get('unk_token')\n",
    "    unknown_id = tokenizer.convert_tokens_to_ids(unknown_token)\n",
    "\n",
    "    model_shortname = model_abb.get(model)\n",
    "\n",
    "    for dataset in language_data_list:\n",
    "        print(dataset)\n",
    "        if dataset in query_list:\n",
    "            testing_text_list = pd.read_csv(f'datasets/{dataset}/data/test.csv')['document'] # text document\n",
    "            print(f\"{dataset} DO NOT have document column \")\n",
    "        else: testing_text_list = pd.read_csv(f'datasets/{dataset}/data/test.csv')['text'] # text document\n",
    "\n",
    "        token_id_list = []\n",
    "        total_text_len = 0\n",
    "        for i, text in enumerate(testing_text_list):\n",
    "\n",
    "            if if_chinese: text_length = len(remove_english(text)) # for chinese only\n",
    "            else: text_length = len(text.split(' ')) # \n",
    "            total_text_len += text_length\n",
    "\n",
    "            one_text_id = tokenizer(text)['input_ids']\n",
    "            token_id_list = token_id_list + one_text_id\n",
    "\n",
    "            # if text_length > len(one_text_id)-2:\n",
    "                # print(\"\".center(50, \"-\"))\n",
    "                # print(text, text_length,  len(one_text_id),len(one_text_id)-2)\n",
    "                # print(one_text_id)\n",
    "\n",
    "        total_token_len = len(token_id_list)-(len(testing_text_list)*2)  # remove the first and the last id\n",
    "        split_ratio = total_token_len/total_text_len\n",
    "        # print(\"\".center(50, \"-\"))\n",
    "        # print('total length :', total_text_len, 'total token length: ', total_token_len, 'average text len:', total_text_len/len(testing_text_list))\n",
    "        # print(f\"==>> split_ratio: {split_ratio}\")\n",
    "\n",
    "\n",
    "        # print( ' ')\n",
    "        unk_num = token_id_list.count(unknown_id)\n",
    "        # print('unknow number : ',unk_num, ' total tokens num: ',len(token_id_list))\n",
    "        # print(f\"==>> unk_ration: {unk_ration}\")\n",
    "        unk_ration = unk_num/len(token_id_list)\n",
    "        \n",
    "        unknow_num.append(unk_num)\n",
    "        unknow_ratio.append(unk_ration)\n",
    "        fertility.append(split_ratio)\n",
    "    model_abb.get(model)\n",
    "    one_data_one_mode_df = pd.DataFrame(list(zip(language_data_list, unknow_num, unknow_ratio, fertility)), \n",
    "                                        columns=['Dataset', f'{model_shortname}_UnkNum', f'{model_shortname}_UnkRatio', f'{model_shortname}_Fertility'])\n",
    "    \n",
    "    one_data_one_mode_df = one_data_one_mode_df.set_index('Dataset')\n",
    "    return one_data_one_mode_df\n",
    "\n",
    "\n",
    "\n",
    "get_one_lan_one_model('xlm-roberta-base', chinese_data_list, chinese_query_list, if_chinese=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "french_csl\n",
      "french_paws\n",
      "french_xnli\n"
     ]
    }
   ],
   "source": [
    "wechsel_french = get_one_lan_one_model('benjamin/roberta-base-wechsel-french', french_data_list, french_query_list, if_chinese=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3557747583354598"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wechsel_french['french_roberta_wechsel_Fertility'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_for_one_lan(model_list, data_list, query_list, if_chinese, lang):\n",
    "    for i, model in enumerate(multi_model_list + model_list):\n",
    "        temp_df = get_one_lan_one_model(model, data_list, query_list, if_chinese=if_chinese)\n",
    "        print(temp_df)\n",
    "        if i == 0:\n",
    "            final_df = temp_df\n",
    "        else: final_df = final_df.join(temp_df)\n",
    "\n",
    "    final_df.to_csv(f'tokenizer_ana/{lang}.csv')\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sst\n",
      "agnews\n",
      "multirc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         mbert_UnkNum  mbert_UnkRatio  mbert_Fertility\n",
      "Dataset                                               \n",
      "sst                 0        0.000000         1.222903\n",
      "agnews              0        0.000000         1.177961\n",
      "multirc          1831        0.001138         1.136510\n",
      "sst\n",
      "agnews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multirc\n",
      "         xlm_roberta_UnkNum  xlm_roberta_UnkRatio  xlm_roberta_Fertility\n",
      "Dataset                                                                 \n",
      "sst                       0                   0.0               1.294136\n",
      "agnews                    0                   0.0               1.339161\n",
      "multirc                   0                   0.0               1.325028\n",
      "sst\n",
      "agnews\n",
      "multirc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         bert_UnkNum  bert_UnkRatio  bert_Fertility\n",
      "Dataset                                            \n",
      "sst                0            0.0        1.123748\n",
      "agnews             0            0.0        1.132458\n",
      "multirc            0            0.0        1.089037\n",
      "sst\n",
      "agnews\n",
      "multirc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         roberta_UnkNum  roberta_UnkRatio  roberta_Fertility\n",
      "Dataset                                                     \n",
      "sst                   0               0.0           1.132667\n",
      "agnews                0               0.0           1.151857\n",
      "multirc               0               0.0           1.090051\n"
     ]
    }
   ],
   "source": [
    "english_df = get_for_one_lan(english_model_list, english_data_list, english_query_list, if_chinese=False, lang='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "french_csl\n",
      "french_paws\n",
      "french_xnli\n",
      "             mbert_UnkNum  mbert_UnkRatio  mbert_Fertility\n",
      "Dataset                                                   \n",
      "french_csl            426        0.004943         1.466762\n",
      "french_paws           117        0.001892         1.425722\n",
      "french_xnli           297        0.001248         1.491164\n",
      "french_csl\n",
      "french_paws\n",
      "french_xnli\n",
      "             xlm_roberta_UnkNum  xlm_roberta_UnkRatio  xlm_roberta_Fertility\n",
      "Dataset                                                                     \n",
      "french_csl                    1              0.000012               1.451057\n",
      "french_paws                   0              0.000000               1.581797\n",
      "french_xnli                   0              0.000000               1.559820\n",
      "french_csl\n",
      "french_paws\n",
      "french_xnli\n",
      "             french_bert_UnkNum  french_bert_UnkRatio  french_bert_Fertility\n",
      "Dataset                                                                     \n",
      "french_csl                   32              0.000394               1.376765\n",
      "french_paws                  90              0.001341               1.555493\n",
      "french_xnli                  36              0.000157               1.435331\n",
      "french_csl\n",
      "french_paws\n",
      "french_xnli\n",
      "             french_roberta_UnkNum  french_roberta_UnkRatio  \\\n",
      "Dataset                                                       \n",
      "french_csl                       0                      0.0   \n",
      "french_paws                      0                      0.0   \n",
      "french_xnli                      0                      0.0   \n",
      "\n",
      "             french_roberta_Fertility  \n",
      "Dataset                                \n",
      "french_csl                   1.313376  \n",
      "french_paws                  1.365151  \n",
      "french_xnli                  1.355728  \n"
     ]
    }
   ],
   "source": [
    "french_df = get_for_one_lan(french_model_list, french_data_list, french_query_list, if_chinese=False, lang='French')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hindi_xnli\n",
      "hindi_bbc_nli\n",
      "hindi_bbc_topic\n",
      "                 mbert_UnkNum  mbert_UnkRatio  mbert_Fertility\n",
      "Dataset                                                       \n",
      "hindi_xnli                 24        0.000083         1.748918\n",
      "hindi_bbc_nli              72        0.001543         2.129390\n",
      "hindi_bbc_topic            72        0.001068         1.844184\n",
      "hindi_xnli\n",
      "hindi_bbc_nli\n",
      "hindi_bbc_topic\n",
      "                 xlm_roberta_UnkNum  xlm_roberta_UnkRatio  \\\n",
      "Dataset                                                     \n",
      "hindi_xnli                        0                   0.0   \n",
      "hindi_bbc_nli                     0                   0.0   \n",
      "hindi_bbc_topic                   0                   0.0   \n",
      "\n",
      "                 xlm_roberta_Fertility  \n",
      "Dataset                                 \n",
      "hindi_xnli                    1.428732  \n",
      "hindi_bbc_nli                 1.676525  \n",
      "hindi_bbc_topic               1.467449  \n",
      "hindi_xnli\n",
      "hindi_bbc_nli\n",
      "hindi_bbc_topic\n",
      "                 hindi_bert_UnkNum  hindi_bert_UnkRatio  hindi_bert_Fertility\n",
      "Dataset                                                                      \n",
      "hindi_xnli                       0                  0.0              1.505565\n",
      "hindi_bbc_nli                    0                  0.0              1.569932\n",
      "hindi_bbc_topic                  0                  0.0              1.572394\n",
      "hindi_xnli\n",
      "hindi_bbc_nli\n",
      "hindi_bbc_topic\n",
      "                 hindi_roberta_UnkNum  hindi_roberta_UnkRatio  \\\n",
      "Dataset                                                         \n",
      "hindi_xnli                          0                     0.0   \n",
      "hindi_bbc_nli                       0                     0.0   \n",
      "hindi_bbc_topic                     0                     0.0   \n",
      "\n",
      "                 hindi_roberta_Fertility  \n",
      "Dataset                                   \n",
      "hindi_xnli                      3.621136  \n",
      "hindi_bbc_nli                   3.836722  \n",
      "hindi_bbc_topic                 3.559943  \n"
     ]
    }
   ],
   "source": [
    "hindi_df = get_for_one_lan(hindi_model_list, hindi_data_list, hindi_query_list, if_chinese=False, lang='hindi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spanish_csl\n",
      "spanish_paws\n",
      "spanish_xnli\n",
      "              mbert_UnkNum  mbert_UnkRatio  mbert_Fertility\n",
      "Dataset                                                    \n",
      "spanish_csl             11        0.000136         1.379627\n",
      "spanish_paws           124        0.002114         1.360527\n",
      "spanish_xnli            13        0.000149         1.367933\n",
      "spanish_csl\n",
      "spanish_paws\n",
      "spanish_xnli\n",
      "              xlm_roberta_UnkNum  xlm_roberta_UnkRatio  xlm_roberta_Fertility\n",
      "Dataset                                                                      \n",
      "spanish_csl                    2              0.000025               1.341765\n",
      "spanish_paws                   0              0.000000               1.470562\n",
      "spanish_xnli                   0              0.000000               1.413404\n",
      "spanish_csl\n",
      "spanish_paws\n",
      "spanish_xnli\n",
      "              BETO_UnkNum  BETO_UnkRatio  BETO_Fertility\n",
      "Dataset                                                 \n",
      "spanish_csl            47       0.000655        1.213836\n",
      "spanish_paws          426       0.007056        1.403396\n",
      "spanish_xnli          118       0.001492        1.231684\n",
      "spanish_csl\n",
      "spanish_paws\n",
      "spanish_xnli\n",
      "              spanish_roberta_UnkNum  spanish_roberta_UnkRatio  \\\n",
      "Dataset                                                          \n",
      "spanish_csl                        0                       0.0   \n",
      "spanish_paws                       0                       0.0   \n",
      "spanish_xnli                       0                       0.0   \n",
      "\n",
      "              spanish_roberta_Fertility  \n",
      "Dataset                                  \n",
      "spanish_csl                    1.201825  \n",
      "spanish_paws                   1.428614  \n",
      "spanish_xnli                   1.238702  \n"
     ]
    }
   ],
   "source": [
    "spanish_df = get_for_one_lan(spanish_model_list, spanish_data_list, spanish_query_list, if_chinese=False, lang='spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mbert_UnkNum</th>\n",
       "      <th>mbert_UnkRatio</th>\n",
       "      <th>mbert_Fertility</th>\n",
       "      <th>xlm_roberta_UnkNum</th>\n",
       "      <th>xlm_roberta_UnkRatio</th>\n",
       "      <th>xlm_roberta_Fertility</th>\n",
       "      <th>BETO_UnkNum</th>\n",
       "      <th>BETO_UnkRatio</th>\n",
       "      <th>BETO_Fertility</th>\n",
       "      <th>spanish_roberta_UnkNum</th>\n",
       "      <th>spanish_roberta_UnkRatio</th>\n",
       "      <th>spanish_roberta_Fertility</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spanish_csl</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>1.379627</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>1.341765</td>\n",
       "      <td>47</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>1.213836</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.201825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spanish_paws</th>\n",
       "      <td>124</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>1.360527</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.470562</td>\n",
       "      <td>426</td>\n",
       "      <td>0.007056</td>\n",
       "      <td>1.403396</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.428614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spanish_xnli</th>\n",
       "      <td>13</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>1.367933</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.413404</td>\n",
       "      <td>118</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>1.231684</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.238702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mbert_UnkNum  mbert_UnkRatio  mbert_Fertility  \\\n",
       "Dataset                                                       \n",
       "spanish_csl             11        0.000136         1.379627   \n",
       "spanish_paws           124        0.002114         1.360527   \n",
       "spanish_xnli            13        0.000149         1.367933   \n",
       "\n",
       "              xlm_roberta_UnkNum  xlm_roberta_UnkRatio  xlm_roberta_Fertility  \\\n",
       "Dataset                                                                         \n",
       "spanish_csl                    2              0.000025               1.341765   \n",
       "spanish_paws                   0              0.000000               1.470562   \n",
       "spanish_xnli                   0              0.000000               1.413404   \n",
       "\n",
       "              BETO_UnkNum  BETO_UnkRatio  BETO_Fertility  \\\n",
       "Dataset                                                    \n",
       "spanish_csl            47       0.000655        1.213836   \n",
       "spanish_paws          426       0.007056        1.403396   \n",
       "spanish_xnli          118       0.001492        1.231684   \n",
       "\n",
       "              spanish_roberta_UnkNum  spanish_roberta_UnkRatio  \\\n",
       "Dataset                                                          \n",
       "spanish_csl                        0                       0.0   \n",
       "spanish_paws                       0                       0.0   \n",
       "spanish_xnli                       0                       0.0   \n",
       "\n",
       "              spanish_roberta_Fertility  \n",
       "Dataset                                  \n",
       "spanish_csl                    1.201825  \n",
       "spanish_paws                   1.428614  \n",
       "spanish_xnli                   1.238702  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mbert_UnkNum</th>\n",
       "      <th>mbert_UnkRatio</th>\n",
       "      <th>mbert_Fertility</th>\n",
       "      <th>xlm_roberta_UnkNum</th>\n",
       "      <th>xlm_roberta_UnkRatio</th>\n",
       "      <th>xlm_roberta_Fertility</th>\n",
       "      <th>bert_UnkNum</th>\n",
       "      <th>bert_UnkRatio</th>\n",
       "      <th>bert_Fertility</th>\n",
       "      <th>roberta_UnkNum</th>\n",
       "      <th>...</th>\n",
       "      <th>french_bert_Fertility</th>\n",
       "      <th>french_roberta_UnkNum</th>\n",
       "      <th>french_roberta_UnkRatio</th>\n",
       "      <th>french_roberta_Fertility</th>\n",
       "      <th>hindi_bert_UnkNum</th>\n",
       "      <th>hindi_bert_UnkRatio</th>\n",
       "      <th>hindi_bert_Fertility</th>\n",
       "      <th>hindi_roberta_UnkNum</th>\n",
       "      <th>hindi_roberta_UnkRatio</th>\n",
       "      <th>hindi_roberta_Fertility</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sst</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.222903</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.294136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.123748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agnews</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.177961</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.339161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.132458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multirc</th>\n",
       "      <td>1831</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>1.136510</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.325028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.089037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChnSentiCorp</th>\n",
       "      <td>343</td>\n",
       "      <td>0.002711</td>\n",
       "      <td>1.043638</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.748148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csl</th>\n",
       "      <td>2172</td>\n",
       "      <td>0.002830</td>\n",
       "      <td>1.122430</td>\n",
       "      <td>446</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.818059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ant</th>\n",
       "      <td>8</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>1.019102</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.814438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spanish_csl</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>1.379627</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>1.341765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spanish_paws</th>\n",
       "      <td>124</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>1.360527</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.470562</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spanish_xnli</th>\n",
       "      <td>13</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>1.367933</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.413404</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>french_csl</th>\n",
       "      <td>426</td>\n",
       "      <td>0.004943</td>\n",
       "      <td>1.466762</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>1.451057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.376765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.313376</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>french_paws</th>\n",
       "      <td>117</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>1.425722</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.581797</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.555493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.365151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>french_xnli</th>\n",
       "      <td>297</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>1.491164</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.559820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.435331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.355728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hindi_xnli</th>\n",
       "      <td>24</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>1.748918</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.428732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.505565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.621136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hindi_bbc_nli</th>\n",
       "      <td>72</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>2.129390</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.676525</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.569932</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.836722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hindi_bbc_topic</th>\n",
       "      <td>72</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>1.844184</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.467449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.572394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.559943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mbert_UnkNum  mbert_UnkRatio  mbert_Fertility  \\\n",
       "Dataset                                                          \n",
       "sst                         0        0.000000         1.222903   \n",
       "agnews                      0        0.000000         1.177961   \n",
       "multirc                  1831        0.001138         1.136510   \n",
       "ChnSentiCorp              343        0.002711         1.043638   \n",
       "csl                      2172        0.002830         1.122430   \n",
       "ant                         8        0.000121         1.019102   \n",
       "spanish_csl                11        0.000136         1.379627   \n",
       "spanish_paws              124        0.002114         1.360527   \n",
       "spanish_xnli               13        0.000149         1.367933   \n",
       "french_csl                426        0.004943         1.466762   \n",
       "french_paws               117        0.001892         1.425722   \n",
       "french_xnli               297        0.001248         1.491164   \n",
       "hindi_xnli                 24        0.000083         1.748918   \n",
       "hindi_bbc_nli              72        0.001543         2.129390   \n",
       "hindi_bbc_topic            72        0.001068         1.844184   \n",
       "\n",
       "                 xlm_roberta_UnkNum  xlm_roberta_UnkRatio  \\\n",
       "Dataset                                                     \n",
       "sst                               0              0.000000   \n",
       "agnews                            0              0.000000   \n",
       "multirc                           0              0.000000   \n",
       "ChnSentiCorp                      5              0.000055   \n",
       "csl                             446              0.000795   \n",
       "ant                               1              0.000018   \n",
       "spanish_csl                       2              0.000025   \n",
       "spanish_paws                      0              0.000000   \n",
       "spanish_xnli                      0              0.000000   \n",
       "french_csl                        1              0.000012   \n",
       "french_paws                       0              0.000000   \n",
       "french_xnli                       0              0.000000   \n",
       "hindi_xnli                        0              0.000000   \n",
       "hindi_bbc_nli                     0              0.000000   \n",
       "hindi_bbc_topic                   0              0.000000   \n",
       "\n",
       "                 xlm_roberta_Fertility  bert_UnkNum  bert_UnkRatio  \\\n",
       "Dataset                                                              \n",
       "sst                           1.294136          0.0            0.0   \n",
       "agnews                        1.339161          0.0            0.0   \n",
       "multirc                       1.325028          0.0            0.0   \n",
       "ChnSentiCorp                  0.748148          NaN            NaN   \n",
       "csl                           0.818059          NaN            NaN   \n",
       "ant                           0.814438          NaN            NaN   \n",
       "spanish_csl                   1.341765          NaN            NaN   \n",
       "spanish_paws                  1.470562          NaN            NaN   \n",
       "spanish_xnli                  1.413404          NaN            NaN   \n",
       "french_csl                    1.451057          NaN            NaN   \n",
       "french_paws                   1.581797          NaN            NaN   \n",
       "french_xnli                   1.559820          NaN            NaN   \n",
       "hindi_xnli                    1.428732          NaN            NaN   \n",
       "hindi_bbc_nli                 1.676525          NaN            NaN   \n",
       "hindi_bbc_topic               1.467449          NaN            NaN   \n",
       "\n",
       "                 bert_Fertility  roberta_UnkNum  ...  french_bert_Fertility  \\\n",
       "Dataset                                          ...                          \n",
       "sst                    1.123748             0.0  ...                    NaN   \n",
       "agnews                 1.132458             0.0  ...                    NaN   \n",
       "multirc                1.089037             0.0  ...                    NaN   \n",
       "ChnSentiCorp                NaN             NaN  ...                    NaN   \n",
       "csl                         NaN             NaN  ...                    NaN   \n",
       "ant                         NaN             NaN  ...                    NaN   \n",
       "spanish_csl                 NaN             NaN  ...                    NaN   \n",
       "spanish_paws                NaN             NaN  ...                    NaN   \n",
       "spanish_xnli                NaN             NaN  ...                    NaN   \n",
       "french_csl                  NaN             NaN  ...               1.376765   \n",
       "french_paws                 NaN             NaN  ...               1.555493   \n",
       "french_xnli                 NaN             NaN  ...               1.435331   \n",
       "hindi_xnli                  NaN             NaN  ...                    NaN   \n",
       "hindi_bbc_nli               NaN             NaN  ...                    NaN   \n",
       "hindi_bbc_topic             NaN             NaN  ...                    NaN   \n",
       "\n",
       "                 french_roberta_UnkNum  french_roberta_UnkRatio  \\\n",
       "Dataset                                                           \n",
       "sst                                NaN                      NaN   \n",
       "agnews                             NaN                      NaN   \n",
       "multirc                            NaN                      NaN   \n",
       "ChnSentiCorp                       NaN                      NaN   \n",
       "csl                                NaN                      NaN   \n",
       "ant                                NaN                      NaN   \n",
       "spanish_csl                        NaN                      NaN   \n",
       "spanish_paws                       NaN                      NaN   \n",
       "spanish_xnli                       NaN                      NaN   \n",
       "french_csl                         0.0                      0.0   \n",
       "french_paws                        0.0                      0.0   \n",
       "french_xnli                        0.0                      0.0   \n",
       "hindi_xnli                         NaN                      NaN   \n",
       "hindi_bbc_nli                      NaN                      NaN   \n",
       "hindi_bbc_topic                    NaN                      NaN   \n",
       "\n",
       "                 french_roberta_Fertility  hindi_bert_UnkNum  \\\n",
       "Dataset                                                        \n",
       "sst                                   NaN                NaN   \n",
       "agnews                                NaN                NaN   \n",
       "multirc                               NaN                NaN   \n",
       "ChnSentiCorp                          NaN                NaN   \n",
       "csl                                   NaN                NaN   \n",
       "ant                                   NaN                NaN   \n",
       "spanish_csl                           NaN                NaN   \n",
       "spanish_paws                          NaN                NaN   \n",
       "spanish_xnli                          NaN                NaN   \n",
       "french_csl                       1.313376                NaN   \n",
       "french_paws                      1.365151                NaN   \n",
       "french_xnli                      1.355728                NaN   \n",
       "hindi_xnli                            NaN                0.0   \n",
       "hindi_bbc_nli                         NaN                0.0   \n",
       "hindi_bbc_topic                       NaN                0.0   \n",
       "\n",
       "                 hindi_bert_UnkRatio  hindi_bert_Fertility  \\\n",
       "Dataset                                                      \n",
       "sst                              NaN                   NaN   \n",
       "agnews                           NaN                   NaN   \n",
       "multirc                          NaN                   NaN   \n",
       "ChnSentiCorp                     NaN                   NaN   \n",
       "csl                              NaN                   NaN   \n",
       "ant                              NaN                   NaN   \n",
       "spanish_csl                      NaN                   NaN   \n",
       "spanish_paws                     NaN                   NaN   \n",
       "spanish_xnli                     NaN                   NaN   \n",
       "french_csl                       NaN                   NaN   \n",
       "french_paws                      NaN                   NaN   \n",
       "french_xnli                      NaN                   NaN   \n",
       "hindi_xnli                       0.0              1.505565   \n",
       "hindi_bbc_nli                    0.0              1.569932   \n",
       "hindi_bbc_topic                  0.0              1.572394   \n",
       "\n",
       "                 hindi_roberta_UnkNum  hindi_roberta_UnkRatio  \\\n",
       "Dataset                                                         \n",
       "sst                               NaN                     NaN   \n",
       "agnews                            NaN                     NaN   \n",
       "multirc                           NaN                     NaN   \n",
       "ChnSentiCorp                      NaN                     NaN   \n",
       "csl                               NaN                     NaN   \n",
       "ant                               NaN                     NaN   \n",
       "spanish_csl                       NaN                     NaN   \n",
       "spanish_paws                      NaN                     NaN   \n",
       "spanish_xnli                      NaN                     NaN   \n",
       "french_csl                        NaN                     NaN   \n",
       "french_paws                       NaN                     NaN   \n",
       "french_xnli                       NaN                     NaN   \n",
       "hindi_xnli                        0.0                     0.0   \n",
       "hindi_bbc_nli                     0.0                     0.0   \n",
       "hindi_bbc_topic                   0.0                     0.0   \n",
       "\n",
       "                 hindi_roberta_Fertility  \n",
       "Dataset                                   \n",
       "sst                                  NaN  \n",
       "agnews                               NaN  \n",
       "multirc                              NaN  \n",
       "ChnSentiCorp                         NaN  \n",
       "csl                                  NaN  \n",
       "ant                                  NaN  \n",
       "spanish_csl                          NaN  \n",
       "spanish_paws                         NaN  \n",
       "spanish_xnli                         NaN  \n",
       "french_csl                           NaN  \n",
       "french_paws                          NaN  \n",
       "french_xnli                          NaN  \n",
       "hindi_xnli                      3.621136  \n",
       "hindi_bbc_nli                   3.836722  \n",
       "hindi_bbc_topic                 3.559943  \n",
       "\n",
       "[15 rows x 36 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [english_df,chinese_df,spanish_df,french_df,hindi_df]\n",
    "\n",
    "result = pd.concat(frames)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('tokenizer_ana/all_fertility.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
